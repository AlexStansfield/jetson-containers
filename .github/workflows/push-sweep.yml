name: Sweep – Platform-Specific Discover → JSON

on:
  push:
    branches: [ dev ]
  workflow_dispatch:
    inputs:
      platforms:
        description: 'Comma-separated runner labels to build (e.g. "orin,thor" or "orin")'
        required: false
        default: "orin,thor"
      subset_mode:
        description: "Package subset: all | prefix | regex | list | first_n | sample_n"
        required: false
        default: "all"
      subset_value:
        description: 'Value for subset (e.g., "a" | "^opencv" | "apex,arrow" | "10")'
        required: false
        default: ""
      exclude_builders:
        description: "Exclude *-builder tags? true/false"
        required: false
        default: "true"
      chunk_size:
        description: 'Packages per job; set "auto" to size from target_jobs_per_platform'
        required: false
        default: "auto"
      target_jobs_per_platform:
        description: "Target total jobs per platform (used when chunk_size=auto)"
        required: false
        default: "60"
      max_parallel:
        description: "Max parallel jobs across ALL platforms"
        required: false
        default: "8"

permissions:
  contents: read

concurrency:
  group: sweep-${{ github.ref }}-platform-discover
  cancel-in-progress: false

defaults:
  run:
    shell: bash -euo pipefail {0}

env:
  INDEX_HOST: jetson-ai-lab.io
  PLATFORMS_DEFAULT: '["orin","thor"]'

# ────────────────────────────────────────────────────────────────────────────────
# 1) Per-platform DISCOVER (runs on each platform runner), uploads plan-<platform>
# ────────────────────────────────────────────────────────────────────────────────
jobs:
  discover:
    name: Discover (${{ matrix.platform }}) & chunk
    strategy:
      fail-fast: false
      matrix:
        platform: ${{ fromJson( github.event.inputs.platforms && format('["{0}"]', replace(github.event.inputs.platforms, ',', '","')) || env.PLATFORMS_DEFAULT ) }}
    runs-on:
      - self-hosted
      - ${{ matrix.platform }}

    outputs:
      # These outputs are mostly for logging/reference; the real handoff is via artifacts.
      platform_${{ matrix.platform }}_total: ${{ steps.count.outputs.total }}
      platform_${{ matrix.platform }}_chunk_size: ${{ steps.size.outputs.chunk_size }}

    steps:
      - name: Checkout (shallow ok)
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: false

      - name: List all packages
        id: list
        run: |
          ./build.sh --list | sed '/^\s*$/d' > all_packages.txt
          echo "Repo total: $(wc -l < all_packages.txt | tr -d ' ')"

      - name: Filter subset (platform-specific list)
        id: subset
        env:
          SUBSET_MODE: ${{ github.event.inputs.subset_mode || 'all' }}
          SUBSET_VALUE: ${{ github.event.inputs.subset_value || '' }}
          EXCLUDE_BUILDERS: ${{ github.event.inputs.exclude_builders || 'true' }}
        run: |
          python3 - <<'PY'
import sys, re, json, random, pathlib
mode  = (sys.argv[1] or "all").lower()
value = (sys.argv[2] or "")
exclude_builders = (sys.argv[3] or "true").lower() in ("1","true","yes","on")
def base(s): return s.split(":",1)[0]
pkgs=[ln.strip() for ln in pathlib.Path("all_packages.txt").read_text().splitlines() if ln.strip()]

if mode == "prefix" and value:
    pkgs = [p for p in pkgs if base(p).lower().startswith(value.lower())]
elif mode == "regex" and value:
    r = re.compile(value, re.I); pkgs = [p for p in pkgs if r.search(base(p))]
elif mode == "list" and value:
    want=set(x.strip() for x in value.split(",") if x.strip())
    pkgs=[p for p in pkgs if base(p) in want or p in want]
elif mode == "first_n":
    n = int(value or "10"); pkgs = pkgs[:n]
elif mode == "sample_n":
    n = int(value or "10"); random.shuffle(pkgs); pkgs = pkgs[:n]
# else "all" → no filter

if exclude_builders:
    pkgs = [p for p in pkgs if not p.endswith("-builder")]

pathlib.Path("filtered.json").write_text(json.dumps(pkgs))
print(f"[subset] mode={mode} value={value} exclude_builders={exclude_builders} selected={len(pkgs)}")
PY
          echo "Preview (first 20):"
          jq -r '.[]' filtered.json | head -n 20 || true

      - name: Count selected
        id: count
        run: |
          echo "total=$(jq 'length' filtered.json)" >> "$GITHUB_OUTPUT"
          echo "Selected=${{ steps.count.outputs.total || '' }}"
        continue-on-error: true

      - name: Compute chunk_size (auto or fixed)
        id: size
        run: |
          CHUNK_RAW="${{ github.event.inputs.chunk_size || 'auto' }}"
          if [[ "$CHUNK_RAW" != "auto" ]]; then
            echo "chunk_size=$CHUNK_RAW" >> "$GITHUB_OUTPUT"
            echo "Using fixed chunk_size=$CHUNK_RAW"
            exit 0
          fi
          M=$(jq 'length' filtered.json)
          TARGET="${{ github.event.inputs.target_jobs_per_platform || '60' }}"
          if [[ -z "$TARGET" || "$TARGET" -lt 1 ]]; then TARGET=60; fi
          K=$(( (M + TARGET - 1) / TARGET ))   # ceil(M / TARGET)
          if [[ "$K" -lt 5 ]]; then K=5; fi     # avoid tiny chunks
          echo "chunk_size=$K" >> "$GITHUB_OUTPUT"
          echo "Auto chunk_size=$K (M=$M, target_jobs_per_platform=$TARGET)"

      - name: Chunk filtered list
        id: chunk
        run: |
          python3 - <<'PY' "${{ steps.size.outputs.chunk_size }}"
import json, sys, pathlib
K=int(sys.argv[1])
pkgs=json.loads(pathlib.Path("filtered.json").read_text())
chunks=[pkgs[i:i+K] for i in range(0,len(pkgs),K)]
pathlib.Path("chunks.json").write_text(json.dumps(chunks))
idx=list(range(len(chunks)))
pathlib.Path("indexes.json").write_text(json.dumps(idx))
print(f"[chunk] filtered={len(pkgs)}, chunk_size={K}, chunks={len(chunks)}")
PY
          echo "Chunks: $(jq 'length' chunks.json)"

      - name: Upload plan for this platform
        uses: actions/upload-artifact@v4
        with:
          name: plan-${{ matrix.platform }}
          path: |
            filtered.json
            chunks.json
            indexes.json
          overwrite: true
          if-no-files-found: error
          retention-days: 7

# ────────────────────────────────────────────────────────────────────────────────
# 2) PLAN: build (platform,index) matrix from discover artifacts
# ────────────────────────────────────────────────────────────────────────────────
  plan:
    name: Plan matrix (platform × chunk)
    needs: [discover]
    runs-on: ubuntu-latest
    outputs:
      include: ${{ steps.make.outputs.include }}
    steps:
      - name: Download all platform plans
        uses: actions/download-artifact@v4
        with:
          path: _plans
          pattern: plan-*

      - name: Build include list
        id: make
        run: |
          python3 - <<'PY'
import json, os, glob, pathlib
pairs=[]
for d in sorted(glob.glob("_plans/plan-*")):
    plat = pathlib.Path(d).name.replace("plan-","",1)
    idx_path = os.path.join(d, "indexes.json")
    if not os.path.exists(idx_path):
        continue
    idx = json.load(open(idx_path))
    for i in idx:
        pairs.append({"platform": plat, "index": i})
print("include=" + json.dumps(pairs))
# expose as GITHUB_OUTPUT
open(os.environ["GITHUB_OUTPUT"],"a").write("include="+json.dumps(pairs))
PY
          echo "Planned pairs: ${{ steps.make.outputs.include }}"

# ────────────────────────────────────────────────────────────────────────────────
# 3) RUN-CHUNKS: builds each chunk on its platform runner
# ────────────────────────────────────────────────────────────────────────────────
  run-chunks:
    name: Build chunk #${{ matrix.index }} on ${{ matrix.platform }}
    needs: [plan]
    if: ${{ fromJson(needs.plan.outputs.include) && (needs.plan.outputs.include != '[]') }}
    strategy:
      fail-fast: false
      max-parallel: ${{ github.event.inputs.max_parallel || 8 }}
      matrix:
        include: ${{ fromJson(needs.plan.outputs.include) }}
    runs-on:
      - self-hosted
      - ${{ matrix.platform }}
    timeout-minutes: 180
    steps:
      - name: Pre-clean (no sudo; avoid hangs)
        run: |
          ROOT="${RUNNER_WORKSPACE}/jetson-containers/jetson-containers"
          rm -rf "${ROOT}/data" "${ROOT}/logs" || true

      - name: Checkout
        uses: actions/checkout@v4
        with:
          clean: false

      - name: Download platform plan
        uses: actions/download-artifact@v4
        with:
          name: plan-${{ matrix.platform }}
          path: _plan

      - name: Prepare packages for this chunk
        run: |
          python3 - <<'PY' "${{ matrix.index }}"
import json, sys, pathlib
idx=int(sys.argv[1])
chunks=json.loads(pathlib.Path("_plan/chunks.json").read_text())
sel=chunks[idx] if idx < len(chunks) else []
pathlib.Path("packages.json").write_text(json.dumps(sel))
print(f"chunk {idx} size: {len(sel)}")
PY

      - name: Build packages sequentially and collect results
        id: build
        env:
          RUNNER_LABEL: ${{ matrix.platform }}
          INDEX_HOST: ${{ env.INDEX_HOST }}
        run: |
          mkdir -p logs
          python3 - <<'PY'
import json, os, subprocess, time, pathlib
pkgs=json.loads(pathlib.Path("packages.json").read_text())
runner=os.environ["RUNNER_LABEL"]
sha=os.environ["GITHUB_SHA"]
repo=os.environ["GITHUB_REPOSITORY"]
run_id=os.environ["GITHUB_RUN_ID"]
attempt=os.environ["GITHUB_RUN_ATTEMPT"]
results=[]
for pkg in pkgs:
  t0=time.time()
  try:
    p=subprocess.run(["./jetson-containers","build","--logs","logs",pkg],
                     stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    log=p.stdout
    status="success" if p.returncode==0 else "build_fail"
  except Exception as e:
    log=str(e); status="build_fail"
  logf=f"logs/{pkg.replace('/','_')}.log"
  pathlib.Path(logf).write_text(log)
  if ":" in pkg:
    name, tag = pkg.split(":",1)
  else:
    name, tag = pkg, "latest"
  results.append({
    "sha": sha,
    "ref": os.environ.get("GITHUB_REF",""),
    "runner": runner,
    "package": name,
    "tag": tag,
    "status": status,
    "duration_s": round(time.time()-t0,1),
    "run_id": run_id,
    "run_attempt": attempt,
    "run_url": f"https://github.com/{repo}/actions/runs/{run_id}",
    "log_relpath": logf
  })
pathlib.Path("results.json").write_text(json.dumps(results, indent=2))
print(f"wrote {len(results)} results")
PY

      - name: Upload chunk results & logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.platform }}-chunk-${{ matrix.index }}-${{ github.run_attempt }}
          path: |
            results.json
            logs/**
          if-no-files-found: ignore
          overwrite: true
          retention-days: 14

# ────────────────────────────────────────────────────────────────────────────────
# 4) COLLATE: merge all chunk results into one results.json
# ────────────────────────────────────────────────────────────────────────────────
  collate:
    name: Collate → results.json (single file)
    needs: [run-chunks]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download all chunk artifacts
        uses: actions/download-artifact@v4
        with:
          path: _in
          pattern: results-*-chunk-*-*

      - name: Merge JSON
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          mapfile -t files < <(find _in -type f -name results.json | sort)
          if [ "${#files[@]}" -eq 0 ]; then
            echo "[]" > results.json
          else
            jq -s '[.[]] | add' "${files[@]}" > results.json
          fi
          echo "Rows: $(jq 'length' results.json)"

      - name: Publish summary (first 50 rows)
        run: |
          echo "## Sweep results for \`${GITHUB_SHA}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Package | Tag | Runner | Status | Duration (s) |" >> "$GITHUB_STEP_SUMMARY"
          echo "|---|---|---|---|---:|" >> "$GITHUB_STEP_SUMMARY"
          jq -r '.[] | "| \(.package) | \(.tag) | \(.runner) | \(.status) | \(.duration_s) |"' results.json | head -n 50 >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "_Full data uploaded as artifact \`sweep-results-${GITHUB_SHA}-${GITHUB_RUN_ATTEMPT}\`._" >> "$GITHUB_STEP_SUMMARY"

      - name: Upload merged results.json
        uses: actions/upload-artifact@v4
        with:
          name: sweep-results-${{ github.sha }}-${{ github.run_attempt }}
          path: results.json
          overwrite: true
          retention-days: 30
