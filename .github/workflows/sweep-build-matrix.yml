name: Sweep – Build Matrix

on:
  push:
    branches: [ dev ]
  workflow_dispatch:
    inputs:
      platforms:
        description: 'Comma-separated runner labels to build (e.g. "orin,thor" or "orin")'
        required: false
        default: "orin,thor"
      subset_mode:
        description: "Package subset: all | prefix | regex | list | first_n | sample_n"
        required: false
        default: "all"
      subset_value:
        description: 'Value for subset (e.g., "a" | "^opencv" | "apex,arrow" | "10")'
        required: false
        default: ""
      exclude_builders:
        description: "Exclude *-builder tags? true/false"
        required: false
        default: "true"
      chunk_size:
        description: 'Packages per job; set "auto" to size from target_jobs_per_platform'
        required: false
        default: "auto"
      target_jobs_per_platform:
        description: "Target total jobs per platform (used when chunk_size=auto)"
        required: false
        default: "60"
      max_parallel:
        description: "Max parallel jobs across ALL platforms"
        required: false
        default: "8"
      pkg_timeout_minutes:
        description: "Per-package timeout (minutes)"
        required: false
        default: "90"

permissions:
  contents: read

concurrency:
  group: sweep-${{ github.ref }}-platform-discover
  cancel-in-progress: false

defaults:
  run:
    shell: bash -euo pipefail {0}

env:
  INDEX_HOST: jetson-ai-lab.io
  PLATFORMS_DEFAULT: '["orin","thor"]'

# ────────────────────────────────────────────────────────────────────────────────
# 1) Per-platform DISCOVER (runs on each platform runner), uploads plan-<platform>
# ────────────────────────────────────────────────────────────────────────────────
jobs:
  init:
    name: Init platforms
    runs-on: ubuntu-latest
    outputs:
      platforms: ${{ steps.set.outputs.platforms }}
    steps:
      - id: set
        run: |
          PLAT="${{ github.event.inputs.platforms }}"
          if [ -z "$PLAT" ]; then
            echo "platforms=${{ env.PLATFORMS_DEFAULT }}" >> "$GITHUB_OUTPUT"
          else
            IFS=',' read -ra ARR <<< "$PLAT"
            JSON=$(printf '"%s",' "${ARR[@]}")
            JSON="[${JSON%,}]"
            echo "platforms=$JSON" >> "$GITHUB_OUTPUT"
          fi
  discover:
    name: Discover (${{ matrix.platform }}) & chunk
    needs: [init]
    strategy:
      fail-fast: false
      matrix:
        platform: ${{ fromJson(needs.init.outputs.platforms) }}
    runs-on:
      - self-hosted
      - ${{ matrix.platform }}

    # outputs are omitted; artifacts are used for handoff

    steps:
      - name: Checkout (shallow ok)
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: false

      - name: List all packages
        id: list
        run: |
          ./build.sh --list | sed '/^\s*$/d' > all_packages.txt
          echo "Repo total: $(wc -l < all_packages.txt | tr -d ' ')"

      - name: Filter subset (platform-specific list)
        id: subset
        env:
          SUBSET_MODE: ${{ github.event.inputs.subset_mode || 'all' }}
          SUBSET_VALUE: ${{ github.event.inputs.subset_value || '' }}
          EXCLUDE_BUILDERS: ${{ github.event.inputs.exclude_builders || 'true' }}
        run: |
          python3 - <<'PY'
          import os, re, json, random, pathlib
          mode  = (os.environ.get("SUBSET_MODE","all") or "all").lower()
          value = os.environ.get("SUBSET_VALUE","")
          exclude_builders = (os.environ.get("EXCLUDE_BUILDERS","true")).lower() in ("1","true","yes","on")

          def base(s): return s.split(":",1)[0]
          pkgs=[ln.strip() for ln in pathlib.Path("all_packages.txt").read_text().splitlines() if ln.strip()]

          if mode == "prefix" and value:
              pkgs = [p for p in pkgs if base(p).lower().startswith(value.lower())]
          elif mode == "regex" and value:
              r = re.compile(value, re.I); pkgs = [p for p in pkgs if r.search(base(p))]
          elif mode == "list" and value:
              want=set(x.strip() for x in value.split(",") if x.strip())
              pkgs=[p for p in pkgs if base(p) in want or p in want]
          elif mode == "first_n":
              n = int(value or "10"); pkgs = pkgs[:n]
          elif mode == "sample_n":
              n = int(value or "10"); random.shuffle(pkgs); pkgs = pkgs[:n]

          if exclude_builders:
              pkgs = [p for p in pkgs if not p.endswith("-builder")]

          pathlib.Path("filtered.json").write_text(json.dumps(pkgs))
          print(f"[subset] mode={mode} value={value} exclude_builders={exclude_builders} selected={len(pkgs)}")
          PY
          echo "Preview (first 20):"
          jq -r '.[]' filtered.json | head -n 20 || true

      - name: Count selected
        id: count
        run: |
          TOTAL=$(jq 'length' filtered.json)
          echo "total=${TOTAL}" >> "$GITHUB_OUTPUT"
          echo "Selected total: ${TOTAL}"
        continue-on-error: true

      - name: Compute chunk_size (auto or fixed)
        id: size
        run: |
          CHUNK_RAW="${{ github.event.inputs.chunk_size || 'auto' }}"
          if [[ "$CHUNK_RAW" != "auto" ]]; then
            echo "chunk_size=$CHUNK_RAW" >> "$GITHUB_OUTPUT"
            echo "Using fixed chunk_size=$CHUNK_RAW"
            exit 0
          fi
          M=$(jq 'length' filtered.json)
          TARGET="${{ github.event.inputs.target_jobs_per_platform || '60' }}"
          if [[ -z "$TARGET" || "$TARGET" -lt 1 ]]; then TARGET=60; fi
          K=$(( (M + TARGET - 1) / TARGET ))   # ceil(M / TARGET)
          if [[ "$K" -lt 5 ]]; then K=5; fi     # avoid tiny chunks
          echo "chunk_size=$K" >> "$GITHUB_OUTPUT"
          echo "Auto chunk_size=$K (M=$M, target_jobs_per_platform=$TARGET)"

      - name: Chunk filtered list
        id: chunk
        env:
          CHUNK_SIZE: ${{ steps.size.outputs.chunk_size }}
        run: |
          python3 - <<'PY'
          import os, json, pathlib
          K = int(os.environ['CHUNK_SIZE'])
          pkgs = json.loads(pathlib.Path('filtered.json').read_text())
          chunks = [pkgs[i:i+K] for i in range(0, len(pkgs), K)] if K > 0 else [pkgs]
          pathlib.Path('chunks.json').write_text(json.dumps(chunks))
          idx = list(range(len(chunks)))
          pathlib.Path('indexes.json').write_text(json.dumps(idx))
          print(f"[chunk] filtered={len(pkgs)}, chunk_size={K}, chunks={len(chunks)}")
          PY

          # Human-friendly log (no jq needed)
          echo "Chunks: $(python3 -c "import json;print(len(json.load(open('chunks.json'))))")"

          # Emit single-line JSON to outputs (compact)
          echo "chunks=$(python3 -c "import json;print(json.dumps(json.load(open('chunks.json'))))")" >> "$GITHUB_OUTPUT"
          echo "indexes=$(python3 -c "import json;print(json.dumps(json.load(open('indexes.json'))))")" >> "$GITHUB_OUTPUT"


      - name: Upload plan for this platform
        uses: actions/upload-artifact@v4
        with:
          name: plan-${{ matrix.platform }}
          path: |
            filtered.json
            chunks.json
            indexes.json
          overwrite: true
          if-no-files-found: error
          retention-days: 7

# ────────────────────────────────────────────────────────────────────────────────
# 2) PLAN: build (platform,index) matrix from discover artifacts
# ────────────────────────────────────────────────────────────────────────────────
  plan:
    name: Plan matrix (platform × chunk)
    needs: [discover]
    runs-on: ubuntu-latest
    outputs:
      include: ${{ steps.make.outputs.include }}
    steps:
      - name: Download all platform plans
        uses: actions/download-artifact@v4
        with:
          path: _plans
          pattern: plan-*

      - name: Build include list
        id: make
        run: |
          python3 - <<'PY'
          import json, os, glob, pathlib
          pairs=[]
          for d in sorted(glob.glob("_plans/plan-*")):
              plat = pathlib.Path(d).name.replace("plan-","",1)
              idx_path = os.path.join(d, "indexes.json")
              if not os.path.exists(idx_path):
                  continue
              idx = json.load(open(idx_path))
              for i in idx:
                  pairs.append({"platform": plat, "index": i})
          print("include=" + json.dumps(pairs))
          # expose as GITHUB_OUTPUT
          open(os.environ["GITHUB_OUTPUT"],"a").write("include="+json.dumps(pairs)+"\n")
          PY

      - name: Show planned pairs
        run:  |
          echo "Planned pairs: ${{ steps.make.outputs.include }}"

      - name: Show detailed build plan
        run: |
          echo "## 📋 Detailed Build Plan" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          # Process each platform's plan
          for plan_dir in _plans/plan-*; do
            if [ -d "$plan_dir" ]; then
              platform=$(basename "$plan_dir" | sed 's/plan-//')
              echo "### Platform: $platform" >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"

              # Show packages in each chunk
              if [ -f "$plan_dir/chunks.json" ]; then
                echo "| Chunk | Packages |" >> "$GITHUB_STEP_SUMMARY"
                echo "|-------|----------|" >> "$GITHUB_STEP_SUMMARY"

                # Use jq to format the chunks nicely
                jq -r 'to_entries[] | "| \(.key) | \(.value | length) packages: \(.value | join(", ")) |"' "$plan_dir/chunks.json" >> "$GITHUB_STEP_SUMMARY"
                echo "" >> "$GITHUB_STEP_SUMMARY"
              fi
            fi
          done

          echo "### Summary" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Total platforms**: $(ls -1 _plans/plan-* | wc -l)" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Total chunks**: $(echo '${{ steps.make.outputs.include }}' | jq 'length')" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Total packages**: $(find _plans -name chunks.json -exec jq 'map(length) | add' {} \; | awk '{sum+=$1} END {print sum}')" >> "$GITHUB_STEP_SUMMARY"

# ────────────────────────────────────────────────────────────────────────────────
# 3) RUN-CHUNKS: builds each chunk on its platform runner
# ────────────────────────────────────────────────────────────────────────────────
  run-chunks:
    name: Build chunk #${{ matrix.index }} on ${{ matrix.platform }}
    needs: [plan]
    if: ${{ fromJson(needs.plan.outputs.include) && (needs.plan.outputs.include != '[]') }}
    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        include: ${{ fromJson(needs.plan.outputs.include) }}
    runs-on:
      - self-hosted
      - ${{ matrix.platform }}
    timeout-minutes: 180
    steps:
      - name: Pre-clean (no sudo; avoid hangs)
        run: |
          ROOT="${RUNNER_WORKSPACE}/jetson-containers/jetson-containers"
          rm -rf "${ROOT}/data" "${ROOT}/logs" || true
          # Clean up any old logs from previous runs
          rm -rf logs/ || true
          mkdir -p logs
          # Remove old root-level build/test directories that might contaminate
          rm -rf build/ test/ || true
          # Clean up any leftover results from previous runs
          rm -f results.json || true

      - name: Checkout
        uses: actions/checkout@v4
        with:
          clean: false

      - name: Download platform plan
        uses: actions/download-artifact@v4
        with:
          name: plan-${{ matrix.platform }}
          path: _plan

      - name: Prepare packages for this chunk
        env:
          IDX: ${{ matrix.index }}
          # If your "Download platform plan" step used `path: _plan`, keep this:
          PLAN_DIR: _plan
          # If you DIDN'T set a path there, the default folder is "plan-${{ matrix.platform }}":
          # PLAN_DIR: plan-${{ matrix.platform }}
        run: |
          python3 - <<'PY'
          import os, json, pathlib
          idx = int(os.environ["IDX"])
          plan_dir = os.environ.get("PLAN_DIR", "_plan")
          chunks_path = pathlib.Path(plan_dir, "chunks.json")
          if not chunks_path.exists():
              raise SystemExit(f"Missing {chunks_path} (check your Download artifact path)")
          chunks = json.loads(chunks_path.read_text())
          sel = chunks[idx] if idx < len(chunks) else []
          pathlib.Path("packages.json").write_text(json.dumps(sel))
          print(f"chunk {idx} size: {len(sel)}")
          PY

      - name: Show build progress info
        run: |
          echo "🚀 Starting build process on ${{ matrix.platform }}"
          echo "📦 Chunk #${{ matrix.index }} will build $(jq 'length' packages.json) packages"
          echo "⏱️  Estimated total time: ~$(($(jq 'length' packages.json) * 5)) minutes"
          echo "💡 Progress updates every 30 seconds - job is NOT stuck!"

      - name: Build packages sequentially and collect results
        id: build
        env:
          RUNNER_LABEL: ${{ matrix.platform }}
          INDEX_HOST: ${{ env.INDEX_HOST }}
          PKG_TIMEOUT_MIN: ${{ github.event.inputs.pkg_timeout_minutes || '90' }}
        run: |
          mkdir -p logs

          # Check available memory and system resources
          echo "🔍 System resource check:"
          free -h
          df -h /
          echo "Available memory: $(free -m | awk 'NR==2{printf "%.1f%%", $3*100/$2 }')"

          python3 - <<'PY'
          import json, os, subprocess, time, pathlib, re, gc
          pkgs = json.loads(pathlib.Path("packages.json").read_text())
          runner = os.environ.get("RUNNER_NAME") or os.environ["RUNNER_LABEL"]
          sha = os.environ["GITHUB_SHA"]
          repo = os.environ["GITHUB_REPOSITORY"]
          run_id = os.environ["GITHUB_RUN_ID"]
          attempt = os.environ["GITHUB_RUN_ATTEMPT"]
          timeout_min = int(os.environ.get("PKG_TIMEOUT_MIN", "90") or "90")
          timeout_s = max(60, timeout_min * 60)
          # Per-package timeout to prevent infinite loops (e.g., block_sparse_attn)
          pkg_timeout_s = min(1200, timeout_s // 3)  # Max 20 minutes per package
          results = []
          total = len(pkgs)

          def check_memory():
            try:
              with open('/proc/meminfo', 'r') as f:
                meminfo = f.read()
              for line in meminfo.split('\n'):
                if 'MemAvailable:' in line:
                  available = int(line.split()[1]) // 1024  # Convert to MB
                  return available
            except:
              return None
            return None
          for idx, pkg in enumerate(pkgs, 1):
            t0 = time.time()
            base, tag = (pkg.split(":", 1) + ["latest"])[:2] if ":" in pkg else (pkg, "latest")
            safe_base = re.sub(r'[^A-Za-z0-9._-]+', '_', base.replace('/', '_'))
            safe_pkg = re.sub(r'[^A-Za-z0-9._-]+', '_', pkg.replace('/', '_'))
            dir_name = f"index_{idx-1:03d}_{safe_base}"
            per_dir = f"logs/{dir_name}"
            logf = f"logs/{dir_name}.log"
            pathlib.Path(per_dir).mkdir(parents=True, exist_ok=True)
            status = "build_fail"

            # Check memory before starting build
            mem_available = check_memory()
            if mem_available is not None:
              print(f"[build] ({idx}/{total}) starting {pkg} - Available memory: {mem_available}MB")
              if mem_available < 1000:  # Less than 1GB available
                print(f"⚠️  WARNING: Low memory ({mem_available}MB available). Build may fail.")
            else:
              print(f"[build] ({idx}/{total}) starting {pkg}")

            # Clean up memory before each build
            gc.collect()
            import ctypes
            try:
              ctypes.CDLL("libc.so.6").malloc_trim(0)
            except:
              pass
            try:
              cmd = ["./build.sh", "--logs", per_dir, pkg]
              print(f"[build] cmd: {' '.join(cmd)}")

              # Use PIPE to capture output in real-time
              p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)
              next_hb = time.time() + 30  # More frequent heartbeat every 30s

              # Write output to log file and show in real-time
              with open(logf, "w") as fh:
                last_progress = time.time()
                last_memory_check = time.time()
                last_progress_print = time.time()
                while True:
                  # Read any available output in real-time
                  try:
                    line = p.stdout.readline()
                    if line:
                      fh.write(line)
                      fh.flush()
                      if line.strip():
                        # Only show stage information
                        if "> BUILDING" in line and "│" in line:
                          stage = line.split("│")[1].strip().replace("> BUILDING ", "").strip()
                          print(f"[stage] Currently BUILDING: {stage}", flush=True)
                        elif "> TESTING" in line and "│" in line:
                          stage = line.split("│")[1].strip().replace("> TESTING ", "").strip()
                          print(f"[stage] Currently TESTING:  {stage}", flush=True)
                  except:
                    pass

                  ret = p.poll()
                  now = time.time()
                  if ret is not None:
                    if ret == 0:
                      status = "success"
                    elif ret == 137:
                      status = "oom_killed"
                      with open(logf, "a") as af:
                        af.write(f"\n[OOM] Package '{pkg}' was killed due to out of memory (exit code 137)\n")
                    else:
                      status = "build_fail"
                    break
                  # Check per-package timeout first (prevents infinite loops)
                  if now - t0 > pkg_timeout_s:
                    try:
                      p.terminate()
                    except Exception:
                      pass
                    try:
                      p.wait(timeout=10)
                    except Exception:
                      try:
                        p.kill()
                      except Exception:
                        pass
                    with open(logf, "a") as af:
                      af.write(f"\n[PKG_TIMEOUT] Package '{pkg}' exceeded {pkg_timeout_s}s and was terminated.\n")
                    status = "timeout"
                    break
                  # Check overall timeout
                  elif now - t0 > timeout_s:
                    try:
                      p.terminate()
                    except Exception:
                      pass
                    try:
                      p.wait(timeout=10)
                    except Exception:
                      try:
                        p.kill()
                      except Exception:
                        pass
                    with open(logf, "a") as af:
                      af.write(f"\n[TIMEOUT] Package '{pkg}' exceeded {timeout_s}s and was terminated.\n")
                    status = "timeout"
                    break

                  # Check memory every 30 seconds to detect OOM conditions
                  if now - last_memory_check >= 30:
                    mem_available = check_memory()
                    if mem_available is not None and mem_available < 2000:  # Less than 2GB
                      print(f"⚠️  WARNING: Low memory detected ({mem_available}MB available). Build may fail soon.")
                      if mem_available < 1000:  # Less than 1GB - kill the build
                        print(f"💥 CRITICAL: Memory too low ({mem_available}MB). Terminating build to prevent OOM.")
                        try:
                          p.terminate()
                        except Exception:
                          pass
                        try:
                          p.wait(timeout=5)
                        except Exception:
                          try:
                            p.kill()
                          except Exception:
                            pass
                        with open(logf, "a") as af:
                          af.write(f"\n[OOM_PREVENTION] Package '{pkg}' terminated due to low memory ({mem_available}MB available)\n")
                        status = "oom_killed"
                        break

                    # Add a progress indicator every 30 seconds to ensure visibility
                    if now - last_progress_print >= 30:
                      elapsed = int(now - t0)
                      print(f"[progress] {pkg} running for {elapsed}s - check logs for details", flush=True)
                      last_progress_print = now
                    last_memory_check = now

                  # Only sleep if no output was processed (non-blocking read)
                  # This prevents the massive slowdown from sleeping after every line
                  if not line:
                    time.sleep(0.05)  # Very short sleep only when no output available
            except Exception as e:
              with open(logf, "a") as fh:
                fh.write(f"\n[EXCEPTION] {e}\n")
              status = "build_fail"
            finally:
              # Clean up after each build
              gc.collect()
              try:
                ctypes.CDLL("libc.so.6").malloc_trim(0)
              except:
                pass

              # Add a small delay to allow system cleanup
              if idx < total:  # Don't delay after the last package
                time.sleep(1)  # Reduced delay

              print(f"[build] ({idx}/{total}) finished {pkg} with status={status} in {round(time.time()-t0,1)}s")

              # Check memory after build
              mem_available = check_memory()
              if mem_available is not None:
                print(f"[build] Memory after {pkg}: {mem_available}MB available")

            name, tag = (pkg.split(":", 1) + ["latest"])[:2] if ":" in pkg else (pkg, "latest")

            # Analyze failure point if build failed
            failure_point = "success" if status == "success" else "unknown"
            if status == "build_fail":
              try:
                with open(logf, "r") as f:
                  log_content = f.read()

                # Clean ANSI escape sequences from log content
                import re
                ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
                log_content = ansi_escape.sub('', log_content)

                # Find the last build/test stage that was attempted (chronologically)
                last_phase, last_stage = "unknown", "unknown"

                for line in log_content.split('\n'):
                  if "> BUILDING" in line and "│" in line:
                    stage = line.split("│")[1].strip().replace("> BUILDING ", "").strip()
                    # Clean ANSI escape sequences from stage name
                    stage = ansi_escape.sub('', stage)
                    last_phase, last_stage = "BUILDING", stage
                  elif "> TESTING" in line and "│" in line:
                    stage = line.split("│")[1].strip().replace("> TESTING ", "").strip()
                    # Clean ANSI escape sequences from stage name
                    stage = ansi_escape.sub('', stage)
                    last_phase, last_stage = "TESTING", stage

                # Extract the package name from the stage (last word/chunk)
                if last_stage != "unknown":
                  # Split by common separators and take the last meaningful part
                  parts = last_stage.replace(":", "-").split("-")
                  # Find the last part that looks like a package name (not version numbers)
                  package_name = parts[-1] if parts else last_stage
                else:
                  package_name = "unknown"

                # Determine failure type based on error patterns and phase
                # Only consider it a test failure if we're in TESTING phase AND there are actual error indicators
                if last_phase == "TESTING" and ("failed" in log_content.lower() or "error" in log_content.lower() or "💣" in log_content):
                  failure_point = f"TESTING {package_name}"
                elif "ERROR: No matching distribution found" in log_content:
                  failure_point = f"BUILDING {package_name} (pip install)"
                elif "ERROR: Could not find a version that satisfies" in log_content:
                  failure_point = f"BUILDING {package_name} (pip install)"
                elif "error: command" in log_content and "cmake" in log_content:
                  failure_point = f"BUILDING {package_name} (cmake)"
                elif "error: command" in log_content and "make" in log_content:
                  failure_point = f"BUILDING {package_name} (make)"
                elif "error: command" in log_content and "ninja" in log_content:
                  failure_point = f"BUILDING {package_name} (ninja)"
                elif "python3 setup.py" in log_content and "returned a non-zero exit code" in log_content:
                  failure_point = f"BUILDING {package_name} (python setup)"
                elif "bdist_wheel" in log_content and "returned a non-zero exit code" in log_content:
                  failure_point = f"BUILDING {package_name} (python wheel)"
                elif "docker build" in log_content and "returned non-zero exit status" in log_content:
                  failure_point = f"BUILDING {package_name} (docker build)"
                elif "test" in log_content.lower() and "failed" in log_content.lower():
                  failure_point = f"TESTING  {package_name}"
                elif "timeout" in log_content.lower():
                  failure_point = f"BUILDING {package_name} (timeout)"
                else:
                  # Use the phase information
                  failure_point = f"{last_phase} {package_name}"
              except Exception:
                failure_point = "log analysis failed"
            elif status == "timeout":
              failure_point = "BUILDING (timeout)"
            elif status == "oom_killed":
              failure_point = "BUILDING (out of memory)"
            results.append({
              "sha": sha,
              "ref": os.environ.get("GITHUB_REF", ""),
              "runner": runner,
              "runner_label": os.environ.get("RUNNER_LABEL", ""),
              "package": name,
              "tag": tag,
              "status": status,
              "failure_point": failure_point,
              "duration_s": round(time.time() - t0, 1),
              "run_id": run_id,
              "run_attempt": attempt,
              "run_url": f"https://github.com/{repo}/actions/runs/{run_id}",
              "log_relpath": logf,
              "timestamp": time.time()
            })
          pathlib.Path("results.json").write_text(json.dumps(results, indent=2))
          print(f"wrote {len(results)} results")

          # Final summary
          success_count = sum(1 for r in results if r["status"] == "success")
          fail_count = sum(1 for r in results if r["status"] == "build_fail")
          timeout_count = sum(1 for r in results if r["status"] == "timeout")
          oom_count = sum(1 for r in results if r["status"] == "oom_killed")
          print(f"\n🎉 Build chunk completed!")
          print(f"✅ Success: {success_count}")
          print(f"❌ Failed: {fail_count}")
          print(f"⏰ Timeout: {timeout_count}")
          print(f"💥 OOM Killed: {oom_count}")
          print(f"📊 Total: {len(results)}")

          # Clean up any duplicate root-level log files that might have been created
          for pkg in pkgs:
            base = pkg.split(":", 1)[0] if ":" in pkg else pkg
            root_log = f"logs/{base}.log"
            if pathlib.Path(root_log).exists():
              print(f"Removing duplicate root-level log: {root_log}")
              pathlib.Path(root_log).unlink()

          # Sanitize any additional files created under logs/ that may contain invalid characters for artifacts
          logs_dir = pathlib.Path("logs")
          if logs_dir.exists():
            for p in logs_dir.rglob("*"):
              if p.is_file():
                safe = re.sub(r'["<>\|:*?\r\n]', "_", p.name)
                if safe != p.name:
                  try:
                    p.rename(p.with_name(safe))
                  except Exception as e:
                    # Best effort: if rename fails, continue so at least our own log files are valid
                    pass
          PY

      - name: Upload chunk results & logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.platform }}-chunk-${{ matrix.index }}-${{ github.run_attempt }}
          path: |
            results.json
            logs/index_*.log
            logs/index_*/
          if-no-files-found: ignore
          overwrite: true
          retention-days: 14

# ────────────────────────────────────────────────────────────────────────────────
# 4) COLLATE: merge all chunk results into one results.json
# ────────────────────────────────────────────────────────────────────────────────
  collate:
    name: Collate → results.json (single file)
    needs: [run-chunks]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download all chunk artifacts
        uses: actions/download-artifact@v4
        with:
          path: _in
          pattern: results-*-chunk-*-${{ github.run_attempt }}

      - name: Merge JSON
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          mapfile -t files < <(find _in -type f -name results.json | sort)
          if [ "${#files[@]}" -eq 0 ]; then
            echo "[]" > results.json
          else
            jq -s '[.[]] | add' "${files[@]}" > results.json
          fi
          echo "Rows: $(jq 'length' results.json)"

      - name: Publish summary (first 50 rows)
        run: |
          echo "## Sweep results for \`${GITHUB_SHA}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Package | Tag | Runner | Status | Failure Point | Duration (s) |" >> "$GITHUB_STEP_SUMMARY"
          echo "|---|---|---|---|---|---|" >> "$GITHUB_STEP_SUMMARY"
          jq -r --slurp '.[] | "| \(.package) | \(.tag) | \(.runner) | \(.status) | \(.failure_point) | \(.duration_s) |"' results.json | head -n 50 >> "$GITHUB_STEP_SUMMARY" || true
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "_Full data uploaded as artifact \`sweep-results-${GITHUB_SHA}-${GITHUB_RUN_ATTEMPT}\`._" >> "$GITHUB_STEP_SUMMARY"

      - name: Upload merged results.json
        uses: actions/upload-artifact@v4
        with:
          name: sweep-results-${{ github.sha }}-${{ github.run_attempt }}
          path: results.json
          overwrite: true
          retention-days: 30
