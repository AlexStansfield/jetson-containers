name: Sweep – Build Matrix

on:
  push:
    branches: [ dev ]
  workflow_dispatch:
    inputs:
      platforms:
        description: 'Comma-separated runner labels to build (e.g. "orin,thor" or "orin")'
        required: false
        default: "orin,thor"
      subset_mode:
        description: "Package subset: all | prefix | regex | list | first_n | sample_n"
        required: false
        default: "all"
      subset_value:
        description: 'Value for subset (e.g., "a" | "^opencv" | "apex,arrow" | "10")'
        required: false
        default: ""
      exclude_builders:
        description: "Exclude *-builder tags? true/false"
        required: false
        default: "true"
      chunk_size:
        description: 'Packages per job; set "auto" to size from target_jobs_per_platform'
        required: false
        default: "auto"
      target_jobs_per_platform:
        description: "Target total jobs per platform (used when chunk_size=auto)"
        required: false
        default: "60"
      max_parallel:
        description: "Max parallel jobs across ALL platforms"
        required: false
        default: "8"
      pkg_timeout_minutes:
        description: "Per-package timeout (minutes)"
        required: false
        default: "90"

permissions:
  contents: read

concurrency:
  group: sweep-${{ github.ref }}-platform-discover
  cancel-in-progress: false

defaults:
  run:
    shell: bash -euo pipefail {0}

env:
  INDEX_HOST: jetson-ai-lab.io
  PLATFORMS_DEFAULT: '["orin","thor"]'

# ────────────────────────────────────────────────────────────────────────────────
# 1) Per-platform DISCOVER (runs on each platform runner), uploads plan-<platform>
# ────────────────────────────────────────────────────────────────────────────────
jobs:
  init:
    name: Init platforms
    runs-on: ubuntu-latest
    outputs:
      platforms: ${{ steps.set.outputs.platforms }}
    steps:
      - id: set
        run: |
          PLAT="${{ github.event.inputs.platforms }}"
          if [ -z "$PLAT" ]; then
            echo "platforms=${{ env.PLATFORMS_DEFAULT }}" >> "$GITHUB_OUTPUT"
          else
            IFS=',' read -ra ARR <<< "$PLAT"
            JSON=$(printf '"%s",' "${ARR[@]}")
            JSON="[${JSON%,}]"
            echo "platforms=$JSON" >> "$GITHUB_OUTPUT"
          fi
  discover:
    name: Discover (${{ matrix.platform }}) & chunk
    needs: [init]
    strategy:
      fail-fast: false
      matrix:
        platform: ${{ fromJson(needs.init.outputs.platforms) }}
    runs-on:
      - self-hosted
      - ${{ matrix.platform }}

    # outputs are omitted; artifacts are used for handoff

    steps:
      - name: Checkout (shallow ok)
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: false

      - name: List all packages
        id: list
        run: |
          ./build.sh --list | sed '/^\s*$/d' > all_packages.txt
          echo "Repo total: $(wc -l < all_packages.txt | tr -d ' ')"

      - name: Filter subset (platform-specific list)
        id: subset
        env:
          SUBSET_MODE: ${{ github.event.inputs.subset_mode || 'all' }}
          SUBSET_VALUE: ${{ github.event.inputs.subset_value || '' }}
          EXCLUDE_BUILDERS: ${{ github.event.inputs.exclude_builders || 'true' }}
        run: |
          python3 - <<'PY'
          import os, re, json, random, pathlib
          mode  = (os.environ.get("SUBSET_MODE","all") or "all").lower()
          value = os.environ.get("SUBSET_VALUE","")
          exclude_builders = (os.environ.get("EXCLUDE_BUILDERS","true")).lower() in ("1","true","yes","on")

          def base(s): return s.split(":",1)[0]
          pkgs=[ln.strip() for ln in pathlib.Path("all_packages.txt").read_text().splitlines() if ln.strip()]

          if mode == "prefix" and value:
              pkgs = [p for p in pkgs if base(p).lower().startswith(value.lower())]
          elif mode == "regex" and value:
              r = re.compile(value, re.I); pkgs = [p for p in pkgs if r.search(base(p))]
          elif mode == "list" and value:
              want=set(x.strip() for x in value.split(",") if x.strip())
              pkgs=[p for p in pkgs if base(p) in want or p in want]
          elif mode == "first_n":
              n = int(value or "10"); pkgs = pkgs[:n]
          elif mode == "sample_n":
              n = int(value or "10"); random.shuffle(pkgs); pkgs = pkgs[:n]

          if exclude_builders:
              pkgs = [p for p in pkgs if not p.endswith("-builder")]

          pathlib.Path("filtered.json").write_text(json.dumps(pkgs))
          print(f"[subset] mode={mode} value={value} exclude_builders={exclude_builders} selected={len(pkgs)}")
          PY
          echo "Preview (first 20):"
          jq -r '.[]' filtered.json | head -n 20 || true

      - name: Count selected
        id: count
        run: |
          TOTAL=$(jq 'length' filtered.json)
          echo "total=${TOTAL}" >> "$GITHUB_OUTPUT"
          echo "Selected total: ${TOTAL}"
        continue-on-error: true

      - name: Compute chunk_size (auto or fixed)
        id: size
        run: |
          CHUNK_RAW="${{ github.event.inputs.chunk_size || 'auto' }}"
          if [[ "$CHUNK_RAW" != "auto" ]]; then
            echo "chunk_size=$CHUNK_RAW" >> "$GITHUB_OUTPUT"
            echo "Using fixed chunk_size=$CHUNK_RAW"
            exit 0
          fi
          M=$(jq 'length' filtered.json)
          TARGET="${{ github.event.inputs.target_jobs_per_platform || '60' }}"
          if [[ -z "$TARGET" || "$TARGET" -lt 1 ]]; then TARGET=60; fi
          K=$(( (M + TARGET - 1) / TARGET ))   # ceil(M / TARGET)
          if [[ "$K" -lt 5 ]]; then K=5; fi     # avoid tiny chunks
          echo "chunk_size=$K" >> "$GITHUB_OUTPUT"
          echo "Auto chunk_size=$K (M=$M, target_jobs_per_platform=$TARGET)"

      - name: Chunk filtered list
        id: chunk
        env:
          CHUNK_SIZE: ${{ steps.size.outputs.chunk_size }}
        run: |
          python3 - <<'PY'
          import os, json, pathlib
          K = int(os.environ['CHUNK_SIZE'])
          pkgs = json.loads(pathlib.Path('filtered.json').read_text())
          chunks = [pkgs[i:i+K] for i in range(0, len(pkgs), K)] if K > 0 else [pkgs]
          pathlib.Path('chunks.json').write_text(json.dumps(chunks))
          idx = list(range(len(chunks)))
          pathlib.Path('indexes.json').write_text(json.dumps(idx))
          print(f"[chunk] filtered={len(pkgs)}, chunk_size={K}, chunks={len(chunks)}")
          PY

          # Human-friendly log (no jq needed)
          echo "Chunks: $(python3 -c "import json;print(len(json.load(open('chunks.json'))))")"

          # Emit single-line JSON to outputs (compact)
          echo "chunks=$(python3 -c "import json;print(json.dumps(json.load(open('chunks.json'))))")" >> "$GITHUB_OUTPUT"
          echo "indexes=$(python3 -c "import json;print(json.dumps(json.load(open('indexes.json'))))")" >> "$GITHUB_OUTPUT"


      - name: Upload plan for this platform
        uses: actions/upload-artifact@v4
        with:
          name: plan-${{ matrix.platform }}
          path: |
            filtered.json
            chunks.json
            indexes.json
          overwrite: true
          if-no-files-found: error
          retention-days: 7

# ────────────────────────────────────────────────────────────────────────────────
# 2) PLAN: build (platform,index) matrix from discover artifacts
# ────────────────────────────────────────────────────────────────────────────────
  plan:
    name: Plan matrix (platform × chunk)
    needs: [discover]
    runs-on: ubuntu-latest
    outputs:
      include: ${{ steps.make.outputs.include }}
    steps:
      - name: Download all platform plans
        uses: actions/download-artifact@v4
        with:
          path: _plans
          pattern: plan-*

      - name: Build include list
        id: make
        run: |
          python3 - <<'PY'
          import json, os, glob, pathlib
          pairs=[]
          for d in sorted(glob.glob("_plans/plan-*")):
              plat = pathlib.Path(d).name.replace("plan-","",1)
              idx_path = os.path.join(d, "indexes.json")
              if not os.path.exists(idx_path):
                  continue
              idx = json.load(open(idx_path))
              for i in idx:
                  pairs.append({"platform": plat, "index": i})
          print("include=" + json.dumps(pairs))
          # expose as GITHUB_OUTPUT
          open(os.environ["GITHUB_OUTPUT"],"a").write("include="+json.dumps(pairs)+"\n")
          PY

      - name: Show planned pairs
        run:  |
          echo "Planned pairs: ${{ steps.make.outputs.include }}"

# ────────────────────────────────────────────────────────────────────────────────
# 3) RUN-CHUNKS: builds each chunk on its platform runner
# ────────────────────────────────────────────────────────────────────────────────
  run-chunks:
    name: Build chunk #${{ matrix.index }} on ${{ matrix.platform }}
    needs: [plan]
    if: ${{ fromJson(needs.plan.outputs.include) && (needs.plan.outputs.include != '[]') }}
    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        include: ${{ fromJson(needs.plan.outputs.include) }}
    runs-on:
      - self-hosted
      - ${{ matrix.platform }}
    timeout-minutes: 180
    steps:
      - name: Pre-clean (no sudo; avoid hangs)
        run: |
          ROOT="${RUNNER_WORKSPACE}/jetson-containers/jetson-containers"
          rm -rf "${ROOT}/data" "${ROOT}/logs" || true

      - name: Checkout
        uses: actions/checkout@v4
        with:
          clean: false

      - name: Download platform plan
        uses: actions/download-artifact@v4
        with:
          name: plan-${{ matrix.platform }}
          path: _plan

      - name: Prepare packages for this chunk
        env:
          IDX: ${{ matrix.index }}
          # If your "Download platform plan" step used `path: _plan`, keep this:
          PLAN_DIR: _plan
          # If you DIDN'T set a path there, the default folder is "plan-${{ matrix.platform }}":
          # PLAN_DIR: plan-${{ matrix.platform }}
        run: |
          python3 - <<'PY'
          import os, json, pathlib
          idx = int(os.environ["IDX"])
          plan_dir = os.environ.get("PLAN_DIR", "_plan")
          chunks_path = pathlib.Path(plan_dir, "chunks.json")
          if not chunks_path.exists():
              raise SystemExit(f"Missing {chunks_path} (check your Download artifact path)")
          chunks = json.loads(chunks_path.read_text())
          sel = chunks[idx] if idx < len(chunks) else []
          pathlib.Path("packages.json").write_text(json.dumps(sel))
          print(f"chunk {idx} size: {len(sel)}")
          PY

      - name: Build packages sequentially and collect results
        id: build
        env:
          RUNNER_LABEL: ${{ matrix.platform }}
          INDEX_HOST: ${{ env.INDEX_HOST }}
          PKG_TIMEOUT_MIN: ${{ github.event.inputs.pkg_timeout_minutes || '90' }}
        run: |
          mkdir -p logs
          python3 - <<'PY'
          import json, os, subprocess, time, pathlib, re
          pkgs = json.loads(pathlib.Path("packages.json").read_text())
          runner = os.environ["RUNNER_LABEL"]
          sha = os.environ["GITHUB_SHA"]
          repo = os.environ["GITHUB_REPOSITORY"]
          run_id = os.environ["GITHUB_RUN_ID"]
          attempt = os.environ["GITHUB_RUN_ATTEMPT"]
          timeout_min = int(os.environ.get("PKG_TIMEOUT_MIN", "90") or "90")
          timeout_s = max(60, timeout_min * 60)
          results = []
          for pkg in pkgs:
            t0 = time.time()
            safe_pkg = re.sub(r'[^A-Za-z0-9._-]+', '_', pkg.replace('/', '_'))
            logf = f"logs/{safe_pkg}.log"
            pathlib.Path(pathlib.Path(logf).parent).mkdir(parents=True, exist_ok=True)
            status = "build_fail"
            try:
              with open(logf, "w") as fh:
                p = subprocess.run(["./jetson-containers", "build", "--logs", "logs", pkg],
                                   stdout=fh, stderr=subprocess.STDOUT, text=True, timeout=timeout_s)
                status = "success" if p.returncode == 0 else "build_fail"
            except subprocess.TimeoutExpired as e:
              # Append timeout note to the log
              with open(logf, "a") as fh:
                fh.write(f"\n[TIMEOUT] Package '{pkg}' exceeded {timeout_s}s and was terminated.\n")
              status = "timeout"
            except Exception as e:
              with open(logf, "a") as fh:
                fh.write(f"\n[EXCEPTION] {e}\n")
              status = "build_fail"

            if ":" in pkg:
              name, tag = pkg.split(":", 1)
            else:
              name, tag = pkg, "latest"
            results.append({
              "sha": sha,
              "ref": os.environ.get("GITHUB_REF", ""),
              "runner": runner,
              "package": name,
              "tag": tag,
              "status": status,
              "duration_s": round(time.time() - t0, 1),
              "run_id": run_id,
              "run_attempt": attempt,
              "run_url": f"https://github.com/{repo}/actions/runs/{run_id}",
              "log_relpath": logf
            })
          pathlib.Path("results.json").write_text(json.dumps(results, indent=2))
          print(f"wrote {len(results)} results")

          # Sanitize any additional files created under logs/ that may contain invalid characters for artifacts
          logs_dir = pathlib.Path("logs")
          if logs_dir.exists():
            for p in logs_dir.rglob("*"):
              if p.is_file():
                safe = re.sub(r'["<>\|:*?\r\n]', "_", p.name)
                if safe != p.name:
                  try:
                    p.rename(p.with_name(safe))
                  except Exception as e:
                    # Best effort: if rename fails, continue so at least our own log files are valid
                    pass
          PY

      - name: Upload chunk results & logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.platform }}-chunk-${{ matrix.index }}-${{ github.run_attempt }}
          path: |
            results.json
            logs/**
          if-no-files-found: ignore
          overwrite: true
          retention-days: 14

# ────────────────────────────────────────────────────────────────────────────────
# 4) COLLATE: merge all chunk results into one results.json
# ────────────────────────────────────────────────────────────────────────────────
  collate:
    name: Collate → results.json (single file)
    needs: [run-chunks]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download all chunk artifacts
        uses: actions/download-artifact@v4
        with:
          path: _in
          pattern: results-*-chunk-*-*

      - name: Merge JSON
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          mapfile -t files < <(find _in -type f -name results.json | sort)
          if [ "${#files[@]}" -eq 0 ]; then
            echo "[]" > results.json
          else
            jq -s '[.[]] | add' "${files[@]}" > results.json
          fi
          echo "Rows: $(jq 'length' results.json)"

      - name: Publish summary (first 50 rows)
        run: |
          echo "## Sweep results for \`${GITHUB_SHA}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Package | Tag | Runner | Status | Duration (s) |" >> "$GITHUB_STEP_SUMMARY"
          echo "|---|---|---|---|---:|" >> "$GITHUB_STEP_SUMMARY"
          jq -r '.[] | "| \(.package) | \(.tag) | \(.runner) | \(.status) | \(.duration_s) |"' results.json | head -n 50 >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "_Full data uploaded as artifact \`sweep-results-${GITHUB_SHA}-${GITHUB_RUN_ATTEMPT}\`._" >> "$GITHUB_STEP_SUMMARY"

      - name: Upload merged results.json
        uses: actions/upload-artifact@v4
        with:
          name: sweep-results-${{ github.sha }}-${{ github.run_attempt }}
          path: results.json
          overwrite: true
          retention-days: 30
